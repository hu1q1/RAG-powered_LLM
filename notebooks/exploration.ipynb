{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb22f1f",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ae7d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # (pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially)\n",
    "from tqdm.auto import tqdm # for progress bars, requires !pip install tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cc39bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the page number\n",
    "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
    "        for each page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
    "        text = page.get_text()  # get plain text encoded as UTF-8\n",
    "        text = text.replace(\"\\n\", \" \").strip()\n",
    "        pages_and_texts.append({\"page_number\": page_number + 1,\n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_and_read_pdf(pdf_path=\"Retrieval-AugmentedGenerationRAG-AdvancingAIwithDynamicKnowledgeIntegration.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2439cc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a287faccadc49fa95a21027b63a22fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': 1,\n",
       "  'page_char_count': 2438,\n",
       "  'page_word_count': 376,\n",
       "  'page_sentence_count_raw': 17,\n",
       "  'page_token_count': 609.5,\n",
       "  'text': \"Disclaimer: The views and opinions expressed in this article belong solely to the author and do not necessarily reflect those of ISC2. As organizations increasingly rely on external cloud platforms such as Amazon Web Services (AWS), maintaining a clear understanding of what’s going on with external services is essential. Based on his experience, Jatin Mannepalli CISSP, CCSP, argues that effective logging is one of the most critical aspects of securing any cloud environment. He considers why having the right logs and knowing how to use them can make or break your security posture. When talking about logging, I can't help but think of some real-world incidents that have had a lasting impact. In 2022, Pegasus Airlines experienced unauthorized access to its AWS environment, leading to significant operational disruptions. The 2019 Capital One breach, in which a simple AWS misconfiguration compromised over 100 million records, showing how devastating small mistakes can be. In June 2021, Turkish beauty brand Cosmolog Kozmetik suffered a data leak due to a misconfigured Amazon S3 bucket. Then there’s the Verkada ransomware attack, in which inadequate logging delayed threat detection and response, causing both financial and reputational harm. These incidents highlight a critical truth: that, without robust logging and monitoring, it’s not just your infrastructure at risk, but your organization’s reputation, privacy, and future. I’ve often emphasized to stakeholders how effective logging not only meets compliance requirements like GDPR, SOX or HIPAA, but also inspires confidence among security teams and reduces the likelihood of costly breaches. Centralizing logs in a security information and event management (SIEM) system has been a game-changer for teams I’ve worked with. They provide real-time analysis and detection capabilities, enabling swift incident response and proactive defenses. However, it all starts with understanding which AWS log sources are most important and how to integrate them into your SIEM effectively. Critical AWS Logs and Why They Matter Integrating AWS logs with a SIEM system is not just about checking a box; it’s about creating actionable visibility into your environment. It isn’t easy to secure what you can't see. Below are the log sources I’ve found to be the most impactful in real- February 18, 2025 Am I Logging the Right AWS Log Sources? Tags Register for exam My Account USD $\"},\n",
       " {'page_number': 2,\n",
       "  'page_char_count': 3331,\n",
       "  'page_word_count': 503,\n",
       "  'page_sentence_count_raw': 24,\n",
       "  'page_token_count': 832.75,\n",
       "  'text': \"world scenarios and how they’ve helped me and my teams secure and track AWS environments effectively and efficiently. AWS Service How to Forward Why They Matter AWS CloudTrail Logs Enable CloudTrail logging to an S3 bucket or CloudWatch Logs. Use an S3-triggered Lambda function, CloudWatch Logs subscription filter or Kinesis Firehose to stream logs directly to the SIEM. Configure your SIEM to parse and monitor for suspicious activities like unauthorized API calls or IAM role changes.\\xa0 Whether it’s unauthorized access or suspicious admin changes, they provided the visibility needed to act swiftly while providing non-repudiation. These logs have covered my bases countless times by tracking every action in the AWS accounts.\\xa0\\xa0 AWS Config Logs Use Amazon Kinesis Data Firehose or AWS Lambda to stream logs from an S3 bucket to your SIEM. Ensure secure data transfer with encryption and appropriate IAM permissions. Configure the SIEM for log ingestion, parsing and monitoring of configuration changes, to detect non-compliant states and enforce security standards.\\xa0 These logs have been invaluable for spotting configuration drifts and policy violations before they caused trouble. They’re essential for tracking resource changes and catching misconfigurations, like open security groups or overly permissive IAM roles. VPC Flow Logs Enable VPC Flow Logs to CloudWatch or S3. Use Lambda or a CloudWatch subscription filter to forward logs to your SIEM or leverage Kinesis or SQS for scalable, real-time or batch processing. Ensure logs are parsed and monitored for security events such as non-standard ports, malicious domains or unauthorized DNS server traffic. When analyzing network behavior or investigating anomalies, these logs provide a detailed history of traffic patterns. They’ve been particularly useful for detecting unauthorized access attempts, especially when you are investigating a breach. AWS GuardDuty Logs Export GuardDuty findings to CloudWatch Events or an S3 bucket. Use a Lambda function triggered by these exports to transfer logs to your SIEM for analysis and continuous security monitoring.\\xa0 GuardDuty logs are indispensable for identifying and responding to security threats in AWS. They’ve flagged suspicious activity, like unauthorized API calls, brute- force attempts and traffic from known malicious IPs. GuardDuty's actionable insights have helped me quickly prioritize and address potential threats. AWS S3 Access Logs Enable Server Access Logging in the S3 console and configure a target bucket to store logs. Use an S3 event-triggered Lambda function or AWS Kinesis Firehose to process and send logs to the SIEM. Configure the SIEM to parse and monitor for suspicious activities like unauthorized access or data exfiltration. Once, during an investigation of a suspected unauthorized data access, these logs helped to pinpoint exactly who accessed an S3 bucket and when, saving critical time. AWS WAF Logs Enable WAF logging in the WebACL settings and choose either Kinesis Data Firehose or an S3 bucket as the log destination. Use Kinesis Firehose to stream logs directly to your SIEM or configure a Lambda function triggered by S3 to These logs have been instrumental in improving my approach towards building secure architecture around web-based attacks. They have often provided me with\"},\n",
       " {'page_number': 3,\n",
       "  'page_char_count': 3455,\n",
       "  'page_word_count': 518,\n",
       "  'page_sentence_count_raw': 18,\n",
       "  'page_token_count': 863.75,\n",
       "  'text': 'forward logs. Monitor web-based threats effectively through comprehensive log analysis.\\xa0 the details (http traffic logs) to block malicious activity and fine-tune WAF rules. AWS Lambda Function Logs Enable logging to Amazon CloudWatch and set up a CloudWatch Logs subscription filter or use Kinesis Data Firehose to stream logs to your SIEM. Alternatively, install and configure a Lambda Extension from your SIEM provider to forward logs directly from the Lambda function to the SIEM, bypassing CloudWatch for reduced latency and greater flexibility.\\xa0 Debugging Lambda issues or detecting unauthorized function executions would have been nearly impossible without these logs, given they are server-less and leave no trace except for the log files. They’ve also proven to be critical for auditing and forensic analysis. AWS CloudWatch Logs Ensure necessary logs are captured in relevant Log Groups. Create a Subscription Filter in CloudWatch Logs and choose a Lambda function, Kinesis Data Firehose or Kinesis Stream as the destination to process and send logs to your SIEM. Alternatively, export logs to an S3 bucket and use S3 event notifications to trigger a Lambda function for forwarding.\\xa0 AWS CloudWatch Logs have been very useful for detecting security issues, like unusual login attempts or multiple failed API calls. They’ve helped me in monitoring application behavior, identify suspicious behavior and troubleshoot incidents on the go.\\xa0 AWS Elastic Load Balancer (ELB) Logs Enable ELB access logging in the ELB settings and configure a target S3 bucket or CloudWatch Logs as the log destination. Set up log forwarding using an S3-triggered Lambda function, CloudWatch Logs subscription filter, or Kinesis Data Firehose to stream logs directly to your SIEM. Analyze client requests and backend access for deeper security incident insights.\\xa0 These logs have been key to understanding traffic patterns, identifying anomalies and preparing for DDoS attacks before they escalate and cause any outage. AWS Route 53 and Route 53 Resolver Logs Enable DNS query logging in the Route 53 console for hosted zones and configure Route 53 Resolver query logging for VPCs. Set log destinations to CloudWatch Logs or an S3 bucket. Create a Subscription Filter in CloudWatch Logs or use an S3-triggered Lambda function or Kinesis Data Firehose to stream logs to your SIEM. Monitor for suspicious DNS activities like tunneling or reconnaissance. DNS management and DNS security is often overlooked, but Route 53 logs have often assisted me in uncovering unauthorized DNS queries, misconfigured DNS records, unusual traffic patterns and prevent potential tunneling attacks during an active incident investigation.\\xa0 Quick Tip: For most AWS services, AWS Kinesis Firehose is ideal for high-throughput log forwarding, while Lambda excels in custom log processing. Use Firehose for simplicity and Lambda for flexibility or combine as per your specific needs. Best Practices For AWS Log Integration To maximize the value of AWS logs, also consider these best practices: Comprehensive Coverage: Ensure all critical AWS services are logged to avoid security blind spots Log Normalization: Standardize log formats for easier correlation and actionable insights Real-Time Ingestion: Avoid delays in log forwarding to enable timely incident responses Access Control: Encrypt logs and enforce role-based access to protect sensitive data and prevent unauthorized modifications'},\n",
       " {'page_number': 4,\n",
       "  'page_char_count': 2330,\n",
       "  'page_word_count': 330,\n",
       "  'page_sentence_count_raw': 16,\n",
       "  'page_token_count': 582.5,\n",
       "  'text': 'Quick Links The Center for Cyber Safety & Education ISC2 Careers Community Blog Contact Service and Support Regular Audits: Periodic reviews of logging setups help close gaps, reduce costs, and improve efficiency The Criticality of Logs The breach examples referenced at the start of this article (at Pegasus Airlines, Capital One and Verkada) highlighted the critical role of comprehensive AWS logging. While integrating key AWS log sources into SIEMs enables proactive threat detection and swift response, I’ve also encountered challenges like alert fatigue. Irrelevant log sources can generate excessive false positives, making it harder to focus on real threats. Carefully selecting log sources is key to ensuring meaningful insights without getting overwhelmed. Therefore, it’s important to understand the balance. Artificial intelligence (AI) and machine learning (ML) are not perfect, yet. Nonetheless, they have transformed how I approach logging. By learning from past event logs, AI-powered SIEMs can predict threats (such as DDoS or ransomware) and trigger automatic responses – something traditional systems struggle with. They have often helped me create dynamic baselines which can be a foreign concept for a traditional SIEM. This evolution has pushed SIEM solutions beyond static rules, enabling proactive defense against increasingly complex threats. Effective log management, SIEM integration, and AI/ML have proven to empower security teams to make informed decisions, mitigate risks and keep critical data secure. A holistic logging approach not only protects infrastructure but also supports compliance and business continuity. Make reviewing and optimizing your logging strategy a priority and repetitive – it’s an essential step in staying resilient against evolving threats. JatinMannepalli CISSP, CCSP, has over 10 years of experience in cybersecurity and risk management across IT, finance, management consulting and high-frequency trading sectors. He has held security engineering, architecture, management and consulting roles, with responsibility for designing secure systems, mitigating risks and aligning cybersecurity strategies with business goals. Related Insights CCSP – Certified Cloud Security Professional CCSP Versus AWS Cloud Certs Cloud Exit Strategies: Why and How to Avoid Vendor Lock-in'},\n",
       " {'page_number': 5,\n",
       "  'page_char_count': 477,\n",
       "  'page_word_count': 75,\n",
       "  'page_sentence_count_raw': 7,\n",
       "  'page_token_count': 119.25,\n",
       "  'text': 'Frequently Asked Questions Contact Us Policies and Procedures ISC2 Around the World ISC2 Authorized China Agency ISC2 Japan A safe and secure cyber world © Copyright 1996-2025. ISC2, Inc. All Rights Reserved. All contents of this site constitute the property of ISC2, Inc. and may not be copied, reproduced or distributed without prior written permission. ISC2, CISSP, SSCP, CCSP, CGRC, CSSLP, HCISPP, ISSAP, ISSEP, ISSMP, CC, and CBK are registered marks of ISC2, Inc. Sitemap'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isc2_1 = open_and_read_pdf(pdf_path=\"Am I Logging the Right AWS Log Sources_.pdf\")\n",
    "isc2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7fe6462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b2da75ad1e415591d58c68cee5d140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in tqdm(isc2_1): #isc2_1 is a list of dictionaries \n",
    "    item[\"sentences\"] = item[\"text\"].split(\". \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f1c6deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa7b939e04b4cff801621242144a615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define split size to turn groups of sentences into chunks\n",
    "num_sentence_chunk_size = 7\n",
    "\n",
    "# Create a function that recursively splits a list into desired sizes\n",
    "def split_list(input_list: list, \n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
    "\n",
    "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
    "    \"\"\"\n",
    "    return [input_list[i : (i + slice_size)] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "# Loop through pages and texts and split sentences into chunks\n",
    "for item in tqdm(isc2_1): #isc2_1 is a list of dictionaries \n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb3e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.00</td>\n",
       "      <td>2406.20</td>\n",
       "      <td>360.40</td>\n",
       "      <td>16.40</td>\n",
       "      <td>601.55</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.58</td>\n",
       "      <td>1192.04</td>\n",
       "      <td>178.74</td>\n",
       "      <td>6.11</td>\n",
       "      <td>298.01</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>477.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>119.25</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.00</td>\n",
       "      <td>2330.00</td>\n",
       "      <td>330.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>582.50</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.00</td>\n",
       "      <td>2438.00</td>\n",
       "      <td>376.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>609.50</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.00</td>\n",
       "      <td>3331.00</td>\n",
       "      <td>503.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>832.75</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.00</td>\n",
       "      <td>3455.00</td>\n",
       "      <td>518.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>863.75</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count         5.00             5.00             5.00                     5.00   \n",
       "mean          3.00          2406.20           360.40                    16.40   \n",
       "std           1.58          1192.04           178.74                     6.11   \n",
       "min           1.00           477.00            75.00                     7.00   \n",
       "25%           2.00          2330.00           330.00                    16.00   \n",
       "50%           3.00          2438.00           376.00                    17.00   \n",
       "75%           4.00          3331.00           503.00                    18.00   \n",
       "max           5.00          3455.00           518.00                    24.00   \n",
       "\n",
       "       page_token_count  num_chunks  \n",
       "count              5.00         5.0  \n",
       "mean             601.55         2.8  \n",
       "std              298.01         1.1  \n",
       "min              119.25         1.0  \n",
       "25%              582.50         3.0  \n",
       "50%              609.50         3.0  \n",
       "75%              832.75         3.0  \n",
       "max              863.75         4.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(isc2_1)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10075ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentence_chunks</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2438</td>\n",
       "      <td>376</td>\n",
       "      <td>17</td>\n",
       "      <td>609.50</td>\n",
       "      <td>Disclaimer: The views and opinions expressed i...</td>\n",
       "      <td>[Disclaimer: The views and opinions expressed ...</td>\n",
       "      <td>[[Disclaimer: The views and opinions expressed...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3331</td>\n",
       "      <td>503</td>\n",
       "      <td>24</td>\n",
       "      <td>832.75</td>\n",
       "      <td>world scenarios and how they’ve helped me and ...</td>\n",
       "      <td>[world scenarios and how they’ve helped me and...</td>\n",
       "      <td>[[world scenarios and how they’ve helped me an...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3455</td>\n",
       "      <td>518</td>\n",
       "      <td>18</td>\n",
       "      <td>863.75</td>\n",
       "      <td>forward logs. Monitor web-based threats effect...</td>\n",
       "      <td>[forward logs, Monitor web-based threats effec...</td>\n",
       "      <td>[[forward logs, Monitor web-based threats effe...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2330</td>\n",
       "      <td>330</td>\n",
       "      <td>16</td>\n",
       "      <td>582.50</td>\n",
       "      <td>Quick Links The Center for Cyber Safety &amp; Educ...</td>\n",
       "      <td>[Quick Links The Center for Cyber Safety &amp; Edu...</td>\n",
       "      <td>[[Quick Links The Center for Cyber Safety &amp; Ed...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>477</td>\n",
       "      <td>75</td>\n",
       "      <td>7</td>\n",
       "      <td>119.25</td>\n",
       "      <td>Frequently Asked Questions Contact Us Policies...</td>\n",
       "      <td>[Frequently Asked Questions Contact Us Policie...</td>\n",
       "      <td>[[Frequently Asked Questions Contact Us Polici...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0            1             2438              376                       17   \n",
       "1            2             3331              503                       24   \n",
       "2            3             3455              518                       18   \n",
       "3            4             2330              330                       16   \n",
       "4            5              477               75                        7   \n",
       "\n",
       "   page_token_count                                               text  \\\n",
       "0            609.50  Disclaimer: The views and opinions expressed i...   \n",
       "1            832.75  world scenarios and how they’ve helped me and ...   \n",
       "2            863.75  forward logs. Monitor web-based threats effect...   \n",
       "3            582.50  Quick Links The Center for Cyber Safety & Educ...   \n",
       "4            119.25  Frequently Asked Questions Contact Us Policies...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [Disclaimer: The views and opinions expressed ...   \n",
       "1  [world scenarios and how they’ve helped me and...   \n",
       "2  [forward logs, Monitor web-based threats effec...   \n",
       "3  [Quick Links The Center for Cyber Safety & Edu...   \n",
       "4  [Frequently Asked Questions Contact Us Policie...   \n",
       "\n",
       "                                     sentence_chunks  num_chunks  \n",
       "0  [[Disclaimer: The views and opinions expressed...           3  \n",
       "1  [[world scenarios and how they’ve helped me an...           4  \n",
       "2  [[forward logs, Monitor web-based threats effe...           3  \n",
       "3  [[Quick Links The Center for Cyber Safety & Ed...           3  \n",
       "4  [[Frequently Asked Questions Contact Us Polici...           1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b90f0793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6456989fd8ad4ad6b1ab4e8b9eaac50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into its own item\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(isc2_1):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "        \n",
    "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "        joined_sentence_chunk = \". \".join(sentence_chunk).strip()\n",
    "        # joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get stats about the chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "# How many chunks do we have?\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f1064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 3,\n",
       "  'sentence_chunk': 'forward logs. Monitor web-based threats effectively through comprehensive log analysis.\\xa0 the details (http traffic logs) to block malicious activity and fine-tune WAF rules. AWS Lambda Function Logs Enable logging to Amazon CloudWatch and set up a CloudWatch Logs subscription filter or use Kinesis Data Firehose to stream logs to your SIEM. Alternatively, install and configure a Lambda Extension from your SIEM provider to forward logs directly from the Lambda function to the SIEM, bypassing CloudWatch for reduced latency and greater flexibility.\\xa0 Debugging Lambda issues or detecting unauthorized function executions would have been nearly impossible without these logs, given they are server-less and leave no trace except for the log files. They’ve also proven to be critical for auditing and forensic analysis. AWS CloudWatch Logs Ensure necessary logs are captured in relevant Log Groups. Create a Subscription Filter in CloudWatch Logs and choose a Lambda function, Kinesis Data Firehose or Kinesis Stream as the destination to process and send logs to your SIEM',\n",
       "  'chunk_char_count': 1072,\n",
       "  'chunk_word_count': 162,\n",
       "  'chunk_token_count': 268.0}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "# View a random sample\n",
    "random.sample(pages_and_chunks, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb22b05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.64</td>\n",
       "      <td>848.36</td>\n",
       "      <td>123.86</td>\n",
       "      <td>212.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.28</td>\n",
       "      <td>316.54</td>\n",
       "      <td>45.15</td>\n",
       "      <td>79.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>353.00</td>\n",
       "      <td>47.00</td>\n",
       "      <td>88.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.00</td>\n",
       "      <td>547.25</td>\n",
       "      <td>79.75</td>\n",
       "      <td>136.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.50</td>\n",
       "      <td>955.00</td>\n",
       "      <td>137.00</td>\n",
       "      <td>238.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.75</td>\n",
       "      <td>1058.25</td>\n",
       "      <td>150.25</td>\n",
       "      <td>264.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.00</td>\n",
       "      <td>1278.00</td>\n",
       "      <td>196.00</td>\n",
       "      <td>319.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count        14.00             14.00             14.00              14.00\n",
       "mean          2.64            848.36            123.86             212.09\n",
       "std           1.28            316.54             45.15              79.13\n",
       "min           1.00            353.00             47.00              88.25\n",
       "25%           2.00            547.25             79.75             136.81\n",
       "50%           2.50            955.00            137.00             238.75\n",
       "75%           3.75           1058.25            150.25             264.56\n",
       "max           5.00           1278.00            196.00             319.50"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stats about our chunks\n",
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "928179c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 1,\n",
       "  'sentence_chunk': \"Disclaimer: The views and opinions expressed in this article belong solely to the author and do not necessarily reflect those of ISC2As organizations increasingly rely on external cloud platforms such as Amazon Web Services (AWS), maintaining a clear understanding of what’s going on with external services is essentialBased on his experience, Jatin Mannepalli CISSP, CCSP, argues that effective logging is one of the most critical aspects of securing any cloud environmentHe considers why having the right logs and knowing how to use them can make or break your security postureWhen talking about logging, I can't help but think of some real-world incidents that have had a lasting impactIn 2022, Pegasus Airlines experienced unauthorized access to its AWS environment, leading to significant operational disruptionsThe 2019 Capital One breach, in which a simple AWS misconfiguration compromised over 100 million records, showing how devastating small mistakes can be\",\n",
       "  'chunk_char_count': 968,\n",
       "  'chunk_word_count': 145,\n",
       "  'chunk_token_count': 242.0},\n",
       " {'page_number': 1,\n",
       "  'sentence_chunk': 'In June 2021, Turkish beauty brand Cosmolog Kozmetik suffered a data leak due to a misconfigured Amazon S3 bucketThen there’s the Verkada ransomware attack, in which inadequate logging delayed threat detection and response, causing both financial and reputational harmThese incidents highlight a critical truth: that, without robust logging and monitoring, it’s not just your infrastructure at risk, but your organization’s reputation, privacy, and futureI’ve often emphasized to stakeholders how effective logging not only meets compliance requirements like GDPR, SOX or HIPAA, but also inspires confidence among security teams and reduces the likelihood of costly breachesCentralizing logs in a security information and event management (SIEM) system has been a game-changer for teams I’ve worked withThey provide real-time analysis and detection capabilities, enabling swift incident response and proactive defensesHowever, it all starts with understanding which AWS log sources are most important and how to integrate them into your SIEM effectively',\n",
       "  'chunk_char_count': 1053,\n",
       "  'chunk_word_count': 148,\n",
       "  'chunk_token_count': 263.25}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cabe4e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6468157e42474cd1b00639e73dd4d94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huiqi\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\huiqi\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e1e6a8be8f4a81a226a51d85ab5dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f252c43b3e44fc885ff41e5bc8b94c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da3543ecc724310b4efb66e31879748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186e81cbc9be45fd85b8dceb48c6a314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e798aab22f44b7383f7fb253a38c684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f70de5204544be2b377f2faa475992c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f613dd229a41229b13f0a6a3adc4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198a33a5d3fc4f9eb5ef5b65f91de6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93081cf83a114397b1bd4d0c1e5cad32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaece3106af6497a834eeccef4c368ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=\"cpu\") # choose the device to load the model to (note: GPU will often be *much* faster than CPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36546b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356fa9040f134bd5b06536b7b3fdf380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model.to(\"cpu\")\n",
    "\n",
    "# Embed each chunk one by one\n",
    "for item in tqdm(pages_and_chunks):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39dd6abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks)\n",
    "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c792fe1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Disclaimer: The views and opinions expressed i...</td>\n",
       "      <td>968</td>\n",
       "      <td>145</td>\n",
       "      <td>242.00</td>\n",
       "      <td>[0.059340823, 0.050530076, 0.030840993, -0.029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In June 2021, Turkish beauty brand Cosmolog Ko...</td>\n",
       "      <td>1053</td>\n",
       "      <td>148</td>\n",
       "      <td>263.25</td>\n",
       "      <td>[0.049672864, 0.017929126, 0.020043159, -0.014...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0            1  Disclaimer: The views and opinions expressed i...   \n",
       "1            1  In June 2021, Turkish beauty brand Cosmolog Ko...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0               968               145             242.00   \n",
       "1              1053               148             263.25   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.059340823, 0.050530076, 0.030840993, -0.029...  \n",
       "1  [0.049672864, 0.017929126, 0.020043159, -0.014...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks_and_embeddings_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935583be",
   "metadata": {},
   "source": [
    "### for the below, ignore if you alr have the df of text chunks and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b425a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_and_embeddings_df_v2 = pd.read_csv(embeddings_df_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e26b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Disclaimer: The views and opinions expressed i...</td>\n",
       "      <td>968</td>\n",
       "      <td>145</td>\n",
       "      <td>242.00</td>\n",
       "      <td>[ 5.93408234e-02  5.05300760e-02  3.08409929e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>In June 2021, Turkish beauty brand Cosmolog Ko...</td>\n",
       "      <td>1053</td>\n",
       "      <td>148</td>\n",
       "      <td>263.25</td>\n",
       "      <td>[ 4.96728644e-02  1.79291256e-02  2.00431589e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0            1  Disclaimer: The views and opinions expressed i...   \n",
       "1            1  In June 2021, Turkish beauty brand Cosmolog Ko...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0               968               145             242.00   \n",
       "1              1053               148             263.25   \n",
       "\n",
       "                                           embedding  \n",
       "0  [ 5.93408234e-02  5.05300760e-02  3.08409929e-...  \n",
       "1  [ 4.96728644e-02  1.79291256e-02  2.00431589e-...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks_and_embeddings_df_v2.head(2) #note that previously saving it as .csv file caused some formatting issues in the 'embedding' col as the scientific notation \"e\" is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6b728",
   "metadata": {},
   "source": [
    "### cont. here if ignored the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56346994",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f338e095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 768])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embeddings_df[\"embedding\"].tolist()), dtype=torch.float32).to(\"cpu\")\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c173d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: importance of logs\n",
      "Time take to get scores on 14 embeddings: 0.00243 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.5495, 0.5167, 0.5145, 0.4558, 0.4534]),\n",
       "indices=tensor([ 4,  0, 10,  2,  1]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "# 1. Define the query\n",
    "# Note: This could be anything. But since we're working with a nutrition textbook, we'll stick with nutrition-based queries.\n",
    "query = \"importance of logs\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 2. Embed the query to the same numerical space as the text examples \n",
    "# Note: It's important to embed your query with the same model you embedded your examples with.\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# 3. Get similarity scores with the dot product (we'll time this for fun)\n",
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "# 4. Get the top-k results (we'll keep this to 5)\n",
    "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd8a50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to print wrapped text \n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ebf396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'importance of logs'\n",
      "\n",
      "Results:\n",
      "Score: 0.5495\n",
      "Text:\n",
      "They’re essential for tracking resource changes and catching misconfigurations,\n",
      "like open security groups or overly permissive IAM rolesVPC Flow Logs Enable VPC\n",
      "Flow Logs to CloudWatch or S3Use Lambda or a CloudWatch subscription filter to\n",
      "forward logs to your SIEM or leverage Kinesis or SQS for scalable, real-time or\n",
      "batch processingEnsure logs are parsed and monitored for security events such as\n",
      "non-standard ports, malicious domains or unauthorized DNS server trafficWhen\n",
      "analyzing network behavior or investigating anomalies, these logs provide a\n",
      "detailed history of traffic patternsThey’ve been particularly useful for\n",
      "detecting unauthorized access attempts, especially when you are investigating a\n",
      "breachAWS GuardDuty Logs Export GuardDuty findings to CloudWatch Events or an S3\n",
      "bucket\n",
      "Page number: 2\n",
      "\n",
      "\n",
      "Score: 0.5167\n",
      "Text:\n",
      "Disclaimer: The views and opinions expressed in this article belong solely to\n",
      "the author and do not necessarily reflect those of ISC2As organizations\n",
      "increasingly rely on external cloud platforms such as Amazon Web Services (AWS),\n",
      "maintaining a clear understanding of what’s going on with external services is\n",
      "essentialBased on his experience, Jatin Mannepalli CISSP, CCSP, argues that\n",
      "effective logging is one of the most critical aspects of securing any cloud\n",
      "environmentHe considers why having the right logs and knowing how to use them\n",
      "can make or break your security postureWhen talking about logging, I can't help\n",
      "but think of some real-world incidents that have had a lasting impactIn 2022,\n",
      "Pegasus Airlines experienced unauthorized access to its AWS environment, leading\n",
      "to significant operational disruptionsThe 2019 Capital One breach, in which a\n",
      "simple AWS misconfiguration compromised over 100 million records, showing how\n",
      "devastating small mistakes can be\n",
      "Page number: 1\n",
      "\n",
      "\n",
      "Score: 0.5145\n",
      "Text:\n",
      "Quick Links The Center for Cyber Safety & Education ISC2 Careers Community Blog\n",
      "Contact Service and Support Regular Audits: Periodic reviews of logging setups\n",
      "help close gaps, reduce costs, and improve efficiency The Criticality of Logs\n",
      "The breach examples referenced at the start of this article (at Pegasus\n",
      "Airlines, Capital One and Verkada) highlighted the critical role of\n",
      "comprehensive AWS loggingWhile integrating key AWS log sources into SIEMs\n",
      "enables proactive threat detection and swift response, I’ve also encountered\n",
      "challenges like alert fatigueIrrelevant log sources can generate excessive false\n",
      "positives, making it harder to focus on real threatsCarefully selecting log\n",
      "sources is key to ensuring meaningful insights without getting\n",
      "overwhelmedTherefore, it’s important to understand the balanceArtificial\n",
      "intelligence (AI) and machine learning (ML) are not perfect, yetNonetheless,\n",
      "they have transformed how I approach logging\n",
      "Page number: 4\n",
      "\n",
      "\n",
      "Score: 0.4558\n",
      "Text:\n",
      "Critical AWS Logs and Why They Matter Integrating AWS logs with a SIEM system is\n",
      "not just about checking a box; it’s about creating actionable visibility into\n",
      "your environmentIt isn’t easy to secure what you can't seeBelow are the log\n",
      "sources I’ve found to be the most impactful in real- February 18, 2025 Am I\n",
      "Logging the Right AWS Log Sources? Tags Register for exam My Account USD $\n",
      "Page number: 1\n",
      "\n",
      "\n",
      "Score: 0.4534\n",
      "Text:\n",
      "In June 2021, Turkish beauty brand Cosmolog Kozmetik suffered a data leak due to\n",
      "a misconfigured Amazon S3 bucketThen there’s the Verkada ransomware attack, in\n",
      "which inadequate logging delayed threat detection and response, causing both\n",
      "financial and reputational harmThese incidents highlight a critical truth: that,\n",
      "without robust logging and monitoring, it’s not just your infrastructure at\n",
      "risk, but your organization’s reputation, privacy, and futureI’ve often\n",
      "emphasized to stakeholders how effective logging not only meets compliance\n",
      "requirements like GDPR, SOX or HIPAA, but also inspires confidence among\n",
      "security teams and reduces the likelihood of costly breachesCentralizing logs in\n",
      "a security information and event management (SIEM) system has been a game-\n",
      "changer for teams I’ve worked withThey provide real-time analysis and detection\n",
      "capabilities, enabling swift incident response and proactive defensesHowever, it\n",
      "all starts with understanding which AWS log sources are most important and how\n",
      "to integrate them into your SIEM effectively\n",
      "Page number: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "# Loop through zipped together scores and indicies from torch.topk\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "    # Print the page number too so we can reference the textbook further (and check the results)\n",
    "    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fe33748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
